service Speech {
    // Performs synchronous speech recognition: receive results after all audio
    // has been sent and processed.
    rpc Recognize(RecognizeRequest) returns (RecognizeResponse) {
    }

    // Performs bidirectional streaming speech recognition: receive results while
    // sending audio. This method is only available via the gRPC API (not REST).
    rpc StreamingRecognize(stream StreamingRecognizeRequest) returns (RecognizeResponse) {    
    }
    
    rpc EchoTest(RecognizeTest) returns (RecognizeTestEcho) {
    }
}

message RecognitionConfig {
    // The encoding of the audio data sent in the request.
    //
    // All encodings support only 1 channel (mono) audio, unless the
    // `audio_channel_count` and `enable_separate_recognition_per_channel` fields
    // are set.
    //
    // For best results, the audio source should be captured and transmitted using
    // a lossless encoding (`FLAC` or `LINEAR16`). The accuracy of the speech
    // recognition can be reduced if lossy codecs are used to capture or transmit
    // audio, particularly if background noise is present. Lossy codecs include
    // `MULAW`, `AMR`, `AMR_WB`, `OGG_OPUS`, `SPEEX_WITH_HEADER_BYTE`, `MP3`,
    // and `WEBM_OPUS`.
    //
    // The `FLAC` and `WAV` audio file formats include a header that describes the
    // included audio content. You can request recognition for `WAV` files that
    // contain either `LINEAR16` or `MULAW` encoded audio.
    // If you send `FLAC` or `WAV` audio file format in
    // your request, you do not need to specify an `AudioEncoding`; the audio
    // encoding format is determined from the file header. If you specify
    // an `AudioEncoding` when you send  send `FLAC` or `WAV` audio, the
    // encoding configuration must match the encoding described in the audio
    // header; otherwise the request returns an
    // [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error code.
    enum AudioEncoding {
      // Not specified.
      ENCODING_UNSPECIFIED = 0;
  
      // Uncompressed 16-bit signed little-endian samples (Linear PCM).
      LINEAR16 = 1;
  
      // `FLAC` (Free Lossless Audio
      // Codec) is the recommended encoding because it is
      // lossless--therefore recognition is not compromised--and
      // requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
      // encoding supports 16-bit and 24-bit samples, however, not all fields in
      // `STREAMINFO` are supported.
      FLAC = 2;
  
      // 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
      MULAW = 3;
  
      // Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
      AMR = 4;
  
      // Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
      AMR_WB = 5;
  
      // Opus encoded audio frames in Ogg container
      // ([OggOpus](https://wiki.xiph.org/OggOpus)).
      // `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.
      OGG_OPUS = 6;
  
      // Although the use of lossy encodings is not recommended, if a very low
      // bitrate encoding is required, `OGG_OPUS` is highly preferred over
      // Speex encoding. The [Speex](https://speex.org/)  encoding supported by
      // Cloud Speech API has a header byte in each block, as in MIME type
      // `audio/x-speex-with-header-byte`.
      // It is a variant of the RTP Speex encoding defined in
      // [RFC 5574](https://tools.ietf.org/html/rfc5574).
      // The stream is a sequence of blocks, one block per RTP packet. Each block
      // starts with a byte containing the length of the block, in bytes, followed
      // by one or more frames of Speex data, padded to an integral number of
      // bytes (octets) as specified in RFC 5574. In other words, each RTP header
      // is replaced with a single byte containing the block length. Only Speex
      // wideband is supported. `sample_rate_hertz` must be 16000.
      SPEEX_WITH_HEADER_BYTE = 7;
  
      // Opus encoded audio frames in WebM container
      // ([OggOpus](https://wiki.xiph.org/OggOpus)). `sample_rate_hertz` must be
      // one of 8000, 12000, 16000, 24000, or 48000.
      WEBM_OPUS = 9;
    };
  
    // Encoding of audio data sent in all `RecognitionAudio` messages.
    // This field is optional for `FLAC` and `WAV` audio files and required
    // for all other audio formats. For details, see [AudioEncoding][google.cloud.speech.v1.RecognitionConfig.AudioEncoding].
    required AudioEncoding encoding = 1;
  
    // Sample rate in Hertz of the audio data sent in all
    // `RecognitionAudio` messages. Valid values are: 8000-48000.
    // 16000 is optimal. For best results, set the sampling rate of the audio
    // source to 16000 Hz. If that's not possible, use the native sample rate of
    // the audio source (instead of re-sampling).
    // This field is optional for FLAC and WAV audio files, but is
    // required for all other audio formats. For details, see [AudioEncoding][google.cloud.speech.v1.RecognitionConfig.AudioEncoding].
    required int32 sample_rate_hertz = 2;
  
    // Required. The language of the supplied audio as a
    // [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
    // Example: "en-US".
    // See [Language
    // Support](https://cloud.google.com/speech-to-text/docs/languages) for a list
    // of the currently supported language codes.
    required string language_code = 3 ;
  
    // Maximum number of recognition hypotheses to be returned.
    // Specifically, the maximum number of `SpeechRecognitionAlternative` messages
    // within each `SpeechRecognitionResult`.
    // The server may return fewer than `max_alternatives`.
    // Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
    // one. If omitted, will return a maximum of one.
    optional int32 max_alternatives = 4;
  
    optional bool enable_word_confidence = 5;
  
    optional string model = 6;
  
  }

  // Contains audio data in the encoding specified in the `RecognitionConfig`.
// Either `content` or `uri` must be supplied. Supplying both or neither
// returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
// [content limits](https://cloud.google.com/speech-to-text/quotas#content).
message RecognitionAudio {
    // The audio source, which is either inline content or a Google Cloud
    // Storage uri.
    oneof audio_source {
      // The audio data bytes encoded as specified in
      // `RecognitionConfig`. Note: as with all bytes fields, proto buffers use a
      // pure binary representation, whereas JSON representations use base64.
      bytes content = 1;
  
      // URI that points to a file that contains audio data bytes as specified in
      // `RecognitionConfig`. The file must not be compressed (for example, gzip).
      // Currently, only Google Cloud Storage URIs are
      // supported, which must be specified in the following format:
      // `gs://bucket_name/object_name` (other URI formats return
      // [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
      // [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
      string uri = 2;
    }
  }

// Alternative hypotheses (a.k.a. n-best list).
message SpeechRecognitionAlternative {
    // Transcript text representing the words that the user spoke.
    // In languages that use spaces to separate words, the transcript might have a
    // leading space if it isn't the first result. You can concatenate each result
    // to obtain the full transcript without using a separator.
    optional string transcript = 1;
  
    // The confidence estimate between 0.0 and 1.0. A higher number
    // indicates an estimated greater likelihood that the recognized words are
    // correct. This field is set only for the top alternative of a non-streaming
    // result or, of a streaming result where `is_final=true`.
    // This field is not guaranteed to be accurate and users should not rely on it
    // to be always provided.
    // The default of 0.0 is a sentinel value indicating `confidence` was not set.
    required float confidence = 2;
  
    // A list of word-specific information for each recognized word.
    // Note: When `enable_speaker_diarization` is true, you will see all the words
    // from the beginning of the audio.
    repeated WordInfo words = 3;
  }
  
  // Word-specific information for recognized words.
  message WordInfo {
    // Time offset relative to the beginning of the audio,
    // and corresponding to the start of the spoken word.
    // This field is only set if `enable_word_time_offsets=true` and only
    // in the top hypothesis.
    // This is an experimental feature and the accuracy of the time offset can
    // vary.
    optional int32 start_time = 1;
  
    // Time offset relative to the beginning of the audio,
    // and corresponding to the end of the spoken word.
    // This field is only set if `enable_word_time_offsets=true` and only
    // in the top hypothesis.
    // This is an experimental feature and the accuracy of the time offset can
    // vary.
    optional int32 end_time = 2;
  
    // The word corresponding to this set of information.
    required string word = 3;
  
    // The confidence estimate between 0.0 and 1.0. A higher number
    // indicates an estimated greater likelihood that the recognized words are
    // correct. This field is set only for the top alternative of a non-streaming
    // result or, of a streaming result where `is_final=true`.
    // This field is not guaranteed to be accurate and users should not rely on it
    // to be always provided.
    // The default of 0.0 is a sentinel value indicating `confidence` was not set.
    optional float confidence = 4;
}  
// A speech recognition result corresponding to a portion of the audio.
message SpeechRecognitionResult {
    // May contain one or more recognition hypotheses (up to the
    // maximum specified in `max_alternatives`).
    // These alternatives are ordered in terms of accuracy, with the top (first)
    // alternative being the most probable, as ranked by the recognizer.
    repeated SpeechRecognitionAlternative alternatives = 1;
  
    // Time offset of the end of this result relative to the
    // beginning of the audio.
    optional int32 result_end_time = 4;
  
    // Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag
    // of the language in this result. This language code was detected to have
    // the most likelihood of being spoken in the audio.
    optional string language_code = 5;
  }

// The top-level message sent by the client for the `Recognize` method.
message RecognizeTest {
    // Required. Provides information to the recognizer that specifies how to
    // process the request.
    required string msg = 1;
}
message RecognizeTestEcho {
    // Required. Provides information to the recognizer that specifies how to
    // process the request.
    required string echo = 1;
}
// The top-level message sent by the client for the `Recognize` method.
message RecognizeRequest {
    // Required. Provides information to the recognizer that specifies how to
    // process the request.
    //required RecognitionConfig config = 1;
    
    // Required. The audio data to be recognized.
    //required RecognitionAudio audio = 2;
    required string data = 1;
}
 
// The only message returned to the client by the `Recognize` method. It
// contains the result as zero or more sequential `SpeechRecognitionResult`
// messages.
message RecognizeResponse {
// Sequential list of transcription results corresponding to
// sequential portions of audio.
//repeated SpeechRecognitionResult results = 2;

// When available, billed audio seconds for the corresponding request.
//optional int32 total_billed_time = 3;
    required string text = 1;
}

message StreamingRecognizeRequest {
    // Required. Provides information to the recognizer that specifies how to
    // process the request.
    //required RecognitionConfig config = 1;
    
    // Required. The audio data to be recognized.
    //required RecognitionAudio audio = 2;
    required string data = 1;
}

message StreamingRecognizeResponse {
    // Sequential list of transcription results corresponding to
    // sequential portions of audio.
    //repeated SpeechRecognitionResult results = 2;
    
    // When available, billed audio seconds for the corresponding request.
    //optional int32 total_billed_time = 3;
        required string text = 1;
    }
    